\newpage

# Background: Review of the literature {#background}

The theoretical foundation for the creation and use of word frequency lists rests on the observation, made popular by the linguist George Kingsley Zipf in the 1930s and 40s, that if one were to create a frequency list of words in a large enough text, the first word would occur roughly twice as often as the second word, three times as often as the third word, and so on [-@Zipfpsychobiologylanguage1935; -@ZipfHumanbehaviorprinciple1949].

This exponential distribution is significant because it means that a small number of words make up the bulk of a text, whereas the majority of the words occur very few times[@SorellZipflawvocabulary2012]. Paul Nation, one of the most influential scholars in the field of vocabulary acquisition, has pointed out that Zipf’s Law—as it is has come to be known—can serve as motivation to language learners and teachers, since learning the most common vocabulary in a language covers so much of the communication that naturally occurs [-@NationLearningvocabularyanother2013, p. 34].

This observation guides the entire endeavor of word list creation and use. Though the CHVL is not sorted using raw frequency alone^[The sorting method is explained in the sections [*Objective Criteria*](#objective-criteria), [*Dispersion*](#dispersion), and [*Sort and Export*](#sort-and-export).], the effect of Zipf's law can be easily seen in the listed frequencies that accompany each item on the list.

One level above this theoretical basis lie the theoretical considerations of the process that serve as the structure upon which the CHVL is built. These include corpus size and text type, general vs. specialized lists, word family levels, and objective criteria. Each of these issues will be treated separately throughout this literature review.


## Corpus design

Before designing a word list, a careful, clear plan must be made for the design of the corpus from which the list is extracted. The corpus must be representative of the language context that the word list wishes to analyze. Of course, it is impossible to capture all of the communications that take place in a particular language. For this simple reason, researchers must make do with an approximation of the whole: a bounded corpus of language.

Though the focus of this literature review is the creation of word frequency lists, the truth is that relatively few corpora have been created for this specific purpose. Most corpora have aimed at being general collections that cover the language (usually English) as a whole in an attempt to serve different theoretical and applied uses. Yet despite this broad objective, the creation of corpora has historically revolved around two big questions: (1) how large should the corpus be, and (2) what kinds of texts should it include. These questions are important not only for corpus creation, but also for corpus selection. Both of these points will be addressed here, with the recurring emphasis being corpus use for word list creation.


### Corpus size

Conventional wisdom in corpus creation states that more is better. If a word list is to accurately reflect the frequencies of words in the language as a whole, then a corpus must contain enough text to approximate the overall use of discourse. This line of thinking is equivalent to the maxim in quantitative research that a sample should be as representative of the target population as possible. And in order to maximize the statistical probability of this representation, the sample must be of an appropriate size for the study.

True, larger sample sizes often increase this probability, but they also tend to be more resource-intensive for the researcher. The same is true of corpus size. When creating a vocabulary list, then, what is an "ideal" corpus size?

Corpora composed of millions of tokens are easy to access today. This is especially true of corpora of written material—corpora of spoken language are still comparatively small. And thanks to advances in computing power, it is finally becoming plausible for more researchers without access to extensive resources to use these mega-corpora for the purpose of word list creation.

The first project to create a one-million-token corpus was a joint effort by Henry Kučera and W. Nelson Francis of Brown University to compile a corpus of American English texts printed in 1961 [@KuceraComputationalanalysispresentday1967], known today simply as the *Brown Corpus*. They strived to create a corpus with equal amounts of texts from different sources by randomly selecting 500 passages of 2,000 words each from different published materials found at the Brown University Library and the Providence Athenaeum. This mixed design would be used as a model by many of the corpora created during the next few decades: <!--examples-->. These began to be compiled at increasingly faster rates. Many of these corpora were created—in part—to serve as parallel corpora of different varieties of English.

<!-- Still missing lots of history here. BNC?! -->

As an example of how quickly corpora have grown in recent decades, consider the history of COBUILD. What began in 1980 as a collaboration between Collins Publishing and a group of researchers led by John Sinclair—the Collins Birmingham University International Language Database (COBUILD)—led to the creation of the *Collins Corpus* of 7-million-tokens by 1982. It continued expanding until transforming into the *Bank of English* in the 1990s, which reached 320 million words in 1997. In 2005, as part of the Collins World Web, which also comprises French, German, and Spanish corpora, it reached 2.5 billion words [@CollinsCobuildEnglish2005]. The Collins Corpus now contains over 4.5 billion words [@historyCollinsCOBUILD].

Today, with the use of web-crawling applications that scour the internet and collect text at unprecedented speed, the sky's the limit.The *enTenTen12* corpus is composed of 12 billion English tokens, all of which were collected in 12 days [@JakubicekTenTenCorpusFamily2013]! At what point, then is a corpus sufficiently large for word list creation?

Studies have approached this specific problem by creating multiple frequency lists—from varying sizes of corpora—and then comparing the efficacy of these lists themselves. The way that efficacy is operationalized, however, varies among studies.

Some studies have explored how closely the rankings of items on a word frequency list correlate with reaction times in a lexical decision task—a widely-used procedure in psychological and psycholinguistic research.<!-- sources --> In a lexical decision task, participants are presented with a series of words and non-words, one after the other, and they are asked to judge which is which as quickly as possible. The reaction times are then analyzed for each word. It is generally agreed that the average time it takes participants to react to a word is a reflection of the way the mental lexicon is organized. For our purposes, multiple studies have found that there exists an inverse correlation between word frequency and reaction time on a lexical decision task (@BalotaAreLexicalDecisions1984; @WhitneyPsychologyLanguage1998). In other words, more common words are accessed and recognized more quickly than less common words. Therefore, an effective word frequency list should correspond to and reflect this reality.

<!-- ************************** -->
<!-- CONTINUE PROOFREADING HERE -->
<!-- ************************** -->

This was precisely the approach taken by @BrysbaertMovingKuceraFrancis2009, who compared respond times collected as part of the massive Elexicon Project  (@BalotaEnglishLexiconProject2007) to words on a series of frequency lists made from increasingly larger corpora.<!--Were they the first to do this? I don’t think so, check article.--> The corpora used were all subcorpora extracted from the British National Corpus (BNC). With each subsequent increase in token count, the word list correlated more and more closely with the response times from lexical decision tasks. This observation validates the line of thinking described at the beginning of this section regarding the need for large corpora. Brysbaert and New hoped to find an “ideal” corpus size after which the increase in effectiveness would no longer be significant enough to justify the additional cost of resources. After conducting several regression analyses on the two sets of data, they found that the variance in the response times that could be accounted for by corpus size reached a plateau at about 16 million words. In other words, for corpora with less than 16 million words, the size of the corpus had a significant effect on the correlation between word frequencies and average response times for those words on lexical decision tasks. For corpora with more than 16 million words, the effect of increasing corpus size became considerably more subtle. In the end, they concluded that in order to construct an effective word list for *high-frequency* words, a corpus of about 1 million tokens is needed. However, in order to reach the same effectiveness for *low-frequency* words, a corpus size of at least 16 million words is preferable.

A different, more straightforward methodology is to directly compare word lists made from corpora of different sizes. Rather than judging the “effectiveness” of a list, this approach measures similarities shared between different lists. Hypothetically, doing this at increasing corpus sizes should allow one to find a size after which the variance between lists only minimally decreases. As with the previous approach, the goal here is to find a point at which the benefits of increasing size no longer outweigh the additional needed resources.

Essentially, then, all corpora of sufficient size should result in nearly the same word frequency list—a theory based on a strict interpretation of Zipf’s law applied to all natural language. If the appropriate criteria can be found—Sorell [-@Sorellstudyissuestechniques2013] suggests—then this would, at last, provide a solution to Nation’s [-@NationLearningvocabularyanother2013, p. 24] observation that, problematically, word lists tend to disagree rather drastically on both the words included and their respective ranking.

Inspired by the computational linguistic measure of *rank distance* [@PopescuRankDistanceStylistic2008]—a method for comparing stylistic differences between texts—Sorell developed a variant of this methodology [-@Sorellstudyissuestechniques2013]. First, he used different corpora of the same size to create multiple word lists, one for each corpus, ranked entirely by frequency. He then identified the percentage of words that are *not* shared between each set of two lists. Finally, he averaged these percentages to find the level of variability created at that specific corpus size. The levels of variability he found were remarkably close to each other—despite using a wide variety of entirely different corpora (with no overlap on texts within each one). He then increased the size of each corpus and repeated the process.

In order to calculate this level of variability, Sorell used a modified version of a complex formula that he borrowed from the natural sciences, and called his resulting calculation the *Dice distance*. Though this Sørensen–Dice coefficient that he altered (also known by other names) is widely used in botany and other fields^[It has even been used in corpus linguistics studies before, primarily as a way to measure collocation [@Rychlylexicographerfriendlyassociationscore2008].] to measure similarity in areas and samples of different sizes [@DiceMeasuresAmountEcologic1945; @Sorensenmethodestablishinggroups1948], the frequency lists measured by Sorell were all purposefully of the same size. What this means is that—apparently without realizing it—his *Dice distance* was ultimately just a simple percentage:

$$\frac{\textbf{number of different words between frequency lists}}{\textbf{total size of frequency list}}$$

Regardless of the round-about way he used to calculate it, Sorell's resulting measure for each corpus size—the level of variability—can be accurately described as the average proportion of difference for word lists at that particular corpus size.

Sorell found that a stable list (about 2% variation) of the most frequent 1,000 words, or a reasonably stable list (less than 5% variation) of the most frequent 3,000, words can be created using a corpus of 50 million tokens. In other words, 1,000-type word lists created from different 50-million-token corpora will likely only differ by 20 words. At the 3,000-type level using the same sizes of corpora, the lists will likely vary by less than 150 words. This is a remarkable level of similarity. Expanding the list to 9,000 types will still only have about 4–7% variation, or 360–630 words. Even corpora of 20 million tokens can be considered sufficient in many cases, since they will result in 3,000-type word list with roughly 5% variation, and 9,000-type word list with less than 10% variation.

Taking a similar approach, though with significant variations, Brezina and Gablasova [-@Brezinatherecoregeneral2015] compared four corpora of various sizes: The Lancaster-Oslo-Bergen Corpus (LOB), The BE06 Corpus of British English (BE06), The British National Corpus (BNC), and EnTenTen16. These corpora had respective token sizes of 1 million, 1 million, 100 million, and 12 billion. The word list created from each corpus was, in this case, a combination of frequency and dispersion—a measure that will be discussed in more detail later in this paper. The resulting word lists were then compared, and the percentage of shared vocabulary words calculated. Additionally, the researchers also calculated the correlation between the ranking for each word that was shared between word lists. Contrary to Sorell, Brezina and Gablasova considered this final comparison an important part of understanding the effect of corpus size.

The aim of this study was not to find a corpus size after which the difference was negligible, but rather to find if there was a significant difference between word lists made from corpora of different sizes. The study found a 78%–84% overlap between each of the 3,000–lemma word lists. 71% of the words were shared among all four of the lists. Based on this number, Brezina and Gablasova concluded that regardless of corpus size—at least for anything larger than one million tokens—“similar results” are obtained.

This conclusion differs significantly from Sorell’s, who concluded that a corpus of at least 20 million tokens (though 50 million is preferable) is needed for a stable word list with low variability. These disagreements are primarily the result of a difference in what should be considered “stable.” At 71% vocabulary overlap—which is sufficient for Brezina and Gablasova—870 words were only found in one of the four lists. This is drastically higher than Sorell’s threshold, which at the 3,000-word level varies in roughly 150 words. Note that Nation and Hwang (1995) found a level of overlap similar to Brezina and Gablasova when comparing the GSL, the LOB, and the Brown corpora—a percentage of overlap that they deemed to be not particularly high.<!--get specific quoted description of their findings from their article--> As Nation later put it, “Brezina and Gablasova are a bit too tolerant in accepting that 71% or even 78%-84% overlap is good enough. If roughly one out of every four or five words is different from one list to another, that is a lot of difference” [-@NationMakingusingword2016, p. 100].

Another difference to mention between these two studies is the unit of counting used. Sorell made lists based on *types*, whereas Brezina and Gablasova preferred the use of *lemmas*. I will explain this important distinction in a later section of this review (“Identifying Words”). For now, it is sufficient to say that the effect of these different measures in comparing word lists created from corpora of different sizes has (to my knowledge) not been studies. This is one area that could benefit from further research.

Lastly, the corpora used by Brezina and Gablasova were all-inclusive: each built on its own philosophy on the way that different types of texts should be balanced in a corpus, but all seeking to be representative of English as a whole. This is also true of the corpora used by Brysbaert and New in their study using response times from a lexical decision task. Contrast this with Sorell’s word lists, which were systematically created from corpora that consisted of only one specific text type. Surely, this is a factor to consider in corpus design.

Therefore, having a sufficiently large corpus is important, as demonstrated in this section. But is it enough? How much do the types of texts included in a corpus factor into its effectiveness for word list creation?



### Text types

There’s been a lot of debate about the “best” way to balance a corpus’ text types. This is a major aspect of corpus design, and one worth delving into. At the end of the day, much of it comes down to the purpose of the corpus. When used for the creation of word lists, one must also consider the intended purpose of the word list itself. Is it for general use or for one of many possible specialized uses? More on this in the next section.

In order to design a corpus with different amounts of text types (i.e. narrative, conversational, academic), clear definitions for these text types are necessary. But is there a better way than the use of subjective genres to classify texts?

Or is there a better methodology than simply mixing a bunch of different texts together, with the hope that the resulting word list covers the language as a whole? This is the most common way of creating frequency lists, but it tends to result in a mix of words that have little relevance to any one purpose. Esoteric, academic words in a beginners’ vocabulary list? Science fiction terms in a vocabulary list for business managers? It’s obvious that a list is only as good as the corpus from which it’s made, which is why a clear delineation of different text types and their qualities is critical.

When speaking of corpus balance, I refer to the proportion of different text types that make up a corpus. Published corpora have taken different approaches in this regard, and published word lists have made use of a variety of strategies for balancing the corpora from which they are made. Coxhead’s *Academic Word List* [-@Coxheadnewacademicword2000] was created from a carefully-designed corpus that used equally-sized sub-corpora of texts from different disciplines. This suited the purpose of her word list well, since it was intended to serve students from a variety of disciplines.<!--Other corpora / published word lists?-->





The importance of identifying a taxonomy of text types based on objective criteria: are there distinguishable linguistic differences between an informal correspondence and a narrative work of fiction? What about between a romance and a fantasy novel?


Biber’s early work [-@BiberVariationspeechwriting1988] conducted an analysis of a wide variety of texts using large corpora to tag syntactic markers and other linguistic attributes that could potentially be used to define different types of texts. In this study, he found a series of five categories (each consisting of two opposite ends of a spectrum) in which texts varied: involved vs. informational, narrative, situated vs. elaborated, persuasive, and abstract.<!--NOTE: are these the original labels, or Sorell’s?--> He then conducted a very large study, which he published as a book, (1995) that found eight distinct, recurring patterns of different combinations of these categories. These groupings serve as a linguistically-based taxonomy that divides texts along objective lines, rather than subjective, culturally-defined genres.

Similar but independent studies were conducted for Somali, Korean, Nukulaelae Tuvuluan, Taiwanese, and Spanish [@BiberDimensionsRegisterVariation1995; @JangDimensionsspokenwritten1998]. For each language, a unique set of text types were identified. However, the texts were found to align along similar distinguishing linguistic dimensions as the English texts.

Sorell [-@Sorellstudyissuestechniques2013] sought to simplify Biber’s eight text types into categories suitable for corpora study. He did this by noticing the closely similar ways that some of the text types lined up along Biber’s five linguistic categories, also incorporating some extra-linguistic features, such as shared contexts (e.g. predominantly spoken types). He also dropped Biber’s two smallest text types, deeming them impractical for corpus study and difficult to isolate. In doing this, he came up with four simplified text types: interactive (conversation), general reported exposition (general writing), imaginative narrative (narrative writing), and academic. Regarding this last type, Biber’s study found a nosignificant difference between academic writing in the natural sciences (“scientific exposition”) and the humanities (“learned exposition”)—he found that natural science uses more concrete language, whereas the humanities tend to use more abstract language. However, Sorell sought to unify these for the sake of simplicity, simply leaving their distinction to “a future study” [-@Sorellstudyissuestechniques2013, p. 68]. Sorell acknowledged that his wasn’t the first attempt at simplification of Biber’s text types, a surprisingly similar effort having been made in the *Longman Grammar of Spoken and Written English* [@BiberLongmangrammarspoken1999, p. 16] and the *Longman Student Grammar of Spoken and Written English* [@BiberLongmanstudentgrammar2002, p. 23].

Sorell found that each of his four simplified text types yielded a vocabulary frequency list that was as unique as the linguistic criteria that Biber had used. He also measured how different they were from each other, and found all four to be equidistant from the next in this order: conversation, narrative, general writing, and academic writing (See section on corpus size for an explanation of this measurement). Sorell, therefore, claims that his own study of vocabulary frequency using his simplified text types as a base has “validated Biber’s studies by adding a vocabulary dimension to the description of each of the key text types” [-@Sorellstudyissuestechniques2013, p. 201].




Despite the importance of spoken language—or the conversation text type—for language learners and linguistic studies, the number of conversation corpora that exist, as well as their size, is very limited. This is clearly because of the difficulty of gathering large amounts of spoken data that then needs to be transcribed by hand in order to be analyzed. It is true that speech recognition software has come a long way in recent years, but its rate of error remains too high for research purposes. It has been estimated that it takes 40 hours to professionally transcribe one hour of audio recording, making the task too costly.<!--source?--> For this reason, some researchers have begun looking at alternative sources for a conversation corpus, including the internet and movie subtitles.

New, et al. [-@Newusefilmsubtitles2007] created a 50-million-token corpus of French subtitles. They divided this into four subcorpora, one for each of the type of media from which the subtitles were extracted: French films, English movies, English television series, and non-English-language European films. The reason for using French subtitles from English media is the sheer dominance of English in the film industry. In order to counter-balance the much larger sizes of the two subcorpora extracted from English media, the researchers measured word frequencies for each subcorpora separately, then averaged them to arrive at the final frequency used for their ranked word list.

In order to test the validity of their new approach, New, et al. used two different methods. First, they compared their subtitle word list with word lists created from more traditional corpora. Second, they used lexical decision times—similar to Brysbaert and New [-@BrysbaertMovingKuceraFrancis2009] above—to test the rankings of words on their list.

The first test found a .73 correlation with a classical French spoken corpus, the “Corpus de Référence du Français Parlé” (CRFP; Equipe DELIC, 2004). However, when looking at the specific words and semantic categories that differ the most, it’s clear that most major differences are caused by the monologue-nature of the CRFP. This corpus was created from a large number of interviews (each asking the same questions to the interviewee), whereas movie subtitles tend to be composed primarily of people interacting in conversations. This results in more colloquial expressions having higher frequencies in the subtitle corpus. The nature of movies themselves also played a role, resulting in an overrepresentation of words related to action movies and police matters—words like *tuer* [to kill], *prison* [jail], and *armes* [weapons] (p. 665).

For the second test of the subtitle word list, the researchers used the lexical decision times from two previous experiments. They found that the subtitle list’s ability to predict lexical decision times was at least equally as accurate as the CRFP frequencies or those from a traditional corpus of written French. In many cases, it actually fared much better, surprising even the researchers themselves. However, this latter test was based on the rather small sample sizes of the two previous experiments (234 and 240 words), limiting the reliability of this test.

Picking up on these findings, and expanding the lexical decision task to a much larger sample size, Brysbaert and New [-@BrysbaertMovingKuceraFrancis2009] compiled a corpus of English subtitles (SUBTLEX~US~) and evaluated it as part of their study. This corpus is composed of subtitles from a wide variety of American films since 1900, though a majority are from 1990, as well as a large number of American television series. They found that the subtitle frequencies were especially good at predicting the lexical decision times of short words, often surpassing the accuracy of rankings based on the many written corpora they tested. It had more difficulty explaining the response times of longer words, which are more rarely found in film than in literature. Overall, their own conclusion confirmed that of the New, et al. [-@Newusefilmsubtitles2007] study, that word frequencies derived from subtitle corpora seem to have a clear advantage over other types of corpora.

Though these two studies arrive at the same conclusion regarding the use of subtitles, more research is needed in this area. If, indeed, subtitles can be considered as appropriate sources for corpora of the conversation text type, their availability will open many possibilities previously made nearly impossible by the difficulty of the collection medium.


<!--
- English subtitle corpus by Brysbaert and New (2009)
    - >The two most interesting language registers currently available are Internet discussion groups and subtitles. Both can easily be gathered, and they have the highest correlations with word processing variables. On the basis of the English findings, frequencies based on discussion groups seem to be indicated for words longer than seven letters, whereas for short words subtitle frequencies are better (Table 5).
-->



## List design

Perhaps even more complex than appropriately designing the corpus from which to extract vocabulary for a word list, researchers have found a wide range of variables that play a role in the design of the list itself. Questions addressed in the literature deal with the difference between a general service list and a specialized list, differences in the way that a “word” is defined and measured, different ranking criteria used, and the influence of subjective criteria on list creation, among other issues.



### General use vs. specialized use

Nation [-@NationMakingusingword2016] emphasized the importance of identifying the purpose of a word list before beginning the creation process. He believes that the main purpose of most general-use lists is to select vocabulary that language learners should learn during their first years of study. Though this may be the stated goal of some general-use lists, it is clear that they in fact serve a wide variety of purposes. He rightfully suggests, however, that the goal of serving language learners is far too broad to be very helpful.  Language learners come to the task at different ages, with different language needs, and with different reasons for learning the language. A word list that is useful for adult learners intent on attending university will likely not be helpful for young leaners whose language focuses on animals, colors, and other age-appropriate material. And yet general-use lists are far more common than specialized-use lists. This is largely due to attempt at finding the language’s core vocabulary.




The majority of word lists in use attempt to describe the vocabulary of the language as a whole. They are designed to be broad and all-encompassing so that they can serve any number of uses and scenarios. Essentially, they are lists that are created for general use. This broad nature of general use lists is reflected in the name of the most widely-used word list, West’s *General Service List* (1953). Others include Nation’s BNC/COCA lists, Browne’s *New General Service List* (2014), Brezina and Gablasova’s *New General Service List* [-@Brezinatherecoregeneral2015], and Dang and Webb’s *Essential Word List* [@NationMakingusingword2016].

Another way of understanding general-use lists is that their objective is to find what is often termed the *core* vocabulary. Though not always explicitly stated, the philosophy behind this approach is that the language being used—usually English—has at its center a self-contained lexicon of essential, primary, basic, fundamental vocabulary that then runs through the entire language. There are layers of frequency and increasing complexity beyond this, with regions of specialized language demarcated for specific purposes such as fields of study or external dialects. Still, this core vocabulary is at the center of it all, and the purpose of a word list is to identify what words fall within its boundaries. Sorell [-@Sorellstudyissuestechniques2013] evaluated a number of definitions of core vocabulary found in the literature. He suggests that general use lists, such as West’s GSL, serve as intuitively-selected lists of core written communication, whereas survival vocabulary lists—often found in travel guides or similar materials—are core vocabulary lists of oral communication.

Relatively fewer researchers have created word lists aimed at a more specific purpose or target audience. Specialized-use lists can be designed to only include words that belong to a specific domain, such as a discipline or trade. They can also encompass vocabulary found in a broad range of disciplines, but which are common in a specific context, such as academic texts. In this case, they usually serve as supplements to aid language learners who are already familiar with the core vocabulary of the language.

Perhaps the most well-known example of a specialized-use list is Coxhead’s Academic Word List [-@Coxheadnewacademicword2000], which replaced the University Word List [@Xueuniversitywordlist1984] as the go-to vocabulary list for aspiring students intent on attending an English-speaking university or those entering the academic world. This is considered a *general* academic word list, since it is for academic use in general, and not for a specific discipline.

More specialized lists include those designed for business English courses, or medical English courses. This is sometimes designated *technical vocabulary*. Nation [-@NationMakingusingword2016] explains that technical vocabulary is most often taught after students have mastered general-use vocabulary, and after they have some familiarity with academic vocabulary. Chung and Nation (2003) looked into the nature of a technical vocabulary. By studying specialized words in the fields of anatomy and applied linguistics, they found that a large number of technical words are also found in the language’s core vocabulary, or have a general academic use as well. However, when used in a technical text, these words take on a specialized definition that is particular to that domain. This means that much vocabulary is shared across layers of vocabulary, though they may vary semantically, based on context.



### Identifying words (word family levels)

One of the most essential questions that needs to be answered when designing a word list is how one is defining a *word*. Though this may seem like a straight-forward decision, it requires thorough planning and a solid understanding of the theory behind the decision. Should *jump* and *jumped* be counted as two different words or just one? What about irregular inflections such as *go* and *went*? In an article aimed at raising awareness of what he calls the “*Word* dilemma,”  Gardner points out that the validity of much vocabulary research hinges “on the various ways that researchers have operationalized the construct of *Word* for counting and analysis purposes” [-@GardnerValidatingconstructword2007, p. 242].

The literature has generally come to accept some key terms that are helpful when speaking of the way words are counted. Beginning with the most basic measurement and progressing to the most complex, we can choose to count tokens, types, lemmas, or word families.

Measuring *tokens* means simply measuring the total number of words. The sentence “I like small dogs, big dogs, and every other kind of dog” contains twelve tokens—twelve words in total. Counting *types* refers to the number of separate and distinct words. That is, *dog* and *dog* are the same type, but *dogs* is a different type—even a single difference makes them different types. The sentence above is composed of eleven types. A level above this, the *lemma* includes the stem of the word and its inflected forms, but not any derived forms of the word (derived forms are usually considered a different part of speech). So *do*, *does*, and *did* are all the same lemma, but *doable* is not. This is because *doable* has the derivational affix *-able*, which turns it into an adjective. Francis, et al. define lemma as “a set of lexical forms having the same stem and belonging to the same major word class, differing only in inflection and/or spelling” [-@FrancisFrequencyanalysisEnglish1982, p. 1].

Finally, the term *word family* is used to describe an even more inclusive level than the lemma. However, its precise definition has often varied among researchers. Bauer and Nation [-@BauerWordfamilies1993] sought to rectify this problem through an in-depth classification of English affixes. Borrowing from Thorndike’s [-@ThorndiketeachingEnglishsuffixes1941] study of English suffixes, their grouping was based on a series of eight criteria: frequency, productivity, predictability, regularity of the written form of the base, regularity of the spoken form of the base, regularity of the spelling of the affix, regularity of the spoken form of the affix, and regularity of function [-@BauerWordfamilies1993, pp. 255–56]. They identified seven “levels” of word families, with each successive one including a larger number of affixes, and therefore a larger number of types per word family. One very useful aspect of their particular system is that it places all the previous levels (type, lemma, etc.) within the same framework. Under their schema, a level 1 word family is the same as a type, a level 2 word family is a lemma (including all regular inflected affixes), and level 7 (the highest level) consists of classical roots and affixes beyond what most speakers any longer consider separate affixes.

Nation himself suggests that for the purposes of language learning, these specific family word levels can be used simply “as a starting point as an initial framework of reference” [-@NationMakingusingword2016, p. 36]. That is, they are one interpretation of how to systematically count words for a frequency list. These levels are based on criteria that reflect the needs of language learners, rather than on any psycholinguistic theory of how speakers’ mental lexicon is arranged. Still, the idea of word families aligns closely with theoretical models that dictate morphological decomposition as a constant. These theories propose that words are often deconstructed into independent morphemes in receptive tasks and recognized that way, for example by deconstructing *jumping* into *jump* and *-ing*. At the other end of the spectrum stand theories that would place *jump* and *jumping* as separate lexical entries [@BrysbaertMovingKuceraFrancis2009, pp. 982–83].

Either way, there is strong evidence to suggest that inflected/derived forms and their base forms do affect each other in some way, suggesting that word families are a measure of a real representation in speakers’ mental lexicon. In one such study, Nagy et al. [@NagyMorphologicalfamiliesinternal1989] explored the effect of both inflectional and derivational family frequency during a lexical decision task. They found that both types of morphological relationships lowered word recognition times, leading to the conclusion that inflections and derivational relationships are both represented in the mental lexicon, either through the grouping of related words under the same entry, or through linked entries. However, all the participants were native English speakers, so to what extent do L2 learners’ lexicons reflect the same level of linking?

More recent studies have found that L2 learners’ morphological knowledge and word-building ability are not nearly as developed. Ward and Chuenjundaeng -[@WardSuffixknowledgeAcquisition2009] conducted a study that tested the receptive ability of Thai engineering and doctoral students learning English. They were tested for their knowledge of a series of base words, together with various derived forms of the same words. They found a surprising lack of familiarity with the derived words, even when participants knew the base forms from which they were derived. Similarly, but from a productive and not receptive standpoint, Schmitt and Zimmerman [-@SchmittDerivativeWordForms2002] found that learners could produce only a limited number of derived forms when presented with a word family headword. These results challenge the common assumption that “once the base word or even a derived word is known, the recognition of other members of the family requires little or no extra effort” [@BauerWordfamilies1993, p. 253].

There is evidence [@Mochizukiaffixacquisitionorder2000; @SchmittResearchingvocabularyword1997] to suggest a positive correlation between vocabulary size and morphological knowledge. If this is the case, then using higher-level word families in Bauer and Nation’s framework for word list creation (as is the case in <!--word lists that do this: Nation’s, AWD, etc.-->), may not be appropriate for learners with limited knowledge of vocabulary—the very learners that many of these lists target.

Similarly, a study by Jeon [-@JeonContributionmorphologicalawareness2011] found that L2 learners’ morphological knowledge leads to greater reading comprehension. Since many word lists are designed to increase reading comprehension in learners, it follows that they will likely be used by students without strong word-building abilities. <!--not sure if this paragraph is useful at all-->

Clearly, then, when it comes to creating a word list, the unit of counting needs to fit the purpose and target audience of that list. Brezina and Gablasova [-@Brezinatherecoregeneral2015] contend that Bauer and Nation’s [-@BauerWordfamilies1993] higher word family levels ignore the lack of transparency that exists between many of the entries that would be placed under the same word family. Especially when creating a word list for beginners, Brezina and Gablasova point out that the morphological knowledge of language learners is often not developed enough. Because their New General Service List was created for beginners, and since it is intended to aid vocabulary acquisition for both receptive and productive purposes, Brezina and Gablasova chose the lemma as their unit of measure.

Seeking to quantify the effect of choosing to measure word families as opposed to word types, Sorell [-@Sorellstudyissuestechniques2013] compared the text coverage of frequency lists made from the same four corpora. Each corpus corresponded to one of Sorell’s text types (see [*text types*](#text-types) above). Sorell’s definition of “word families” was a slightly modified version of Bauer and Nation’s [-@BauerWordfamilies1993] sixth level of affix inclusion. He found, as would be expected, that the most frequent word families have a much larger text coverage than the most frequent types. This is especially true when measuring type coverage—the most frequent word families accounted for roughly 4–6 times as many types in each corpus. However, when measuring overall token coverage, the top word families only covered about 3–10% more than the same number of most frequent types. Sorell also found that the most frequent 1,000 word families consisted of 6,557 word types in the general writing corpus. The number was similar in the other text types, though somewhat lower.

<!--How do these labels apply to non-English languages?-->



### Objective design

Many word lists—including some of the most widely-known lists—take what could be termed a semi-objective approach. They begin by creating a list that bases word rankings on statistical measures such as frequency, range, and dispersion. Then, because certain words don't fit the researcher's intuitions, or because some rankings simply seem out of order, the list is tweaked here and there [@NationMakingusingword2016, p. 133].

For example, one common tweak is to group lexical sets together on a list, such as days of the week or numbers. This is true of West's GSL, resulting in a list that "brought a large element of subjectivity into the final product." [@Brezinatherecoregeneral2015, p. 3] West himself laid out his argument as to why such an approach is preferable [-@Westgeneralservicelist1953, pp. ix–x].

Despite a few supposed pedagogical advantages, however, a semi-objective approach (which is therefore also a semi-subjective approach) has important implications for reproducibility. This alone makes it unfit for the present project, since one of the primary goals of this thesis is to present an easily reproducible process than can be use to create vocabulary lists in many different languages. Additionally, the simple fact is that by inserting subjective criteria into the list-creation process, it ceases to be based on the data directly. Rather than letting a particular corpus speak for itself, the whims and opinions of the researcher come into play. This can affect secondary tests that may be performed using the list, such as a lexical decision test.

Some lists that use strictly objective criteria include *Word Frequencies in Written and Spoken English* [@LeechWordFrequenciesWritten2001], Brezine and Gablasova's *New General Service List* [-@Brezinatherecoregeneral2015], and Dang and Webb's *Essential Word List* [@NationMakingusingword2016, pp. 153–67]. This thesis also uses exclusively objective criteria to create the *Conversational Hebrew Vocabulary List*: frequency, range, and dispersion. Let us now discuss each of these in turn.


#### Frequency

Frequency can refer to either raw frequency (sometimes called absolute frequency) or normalized frequency. Raw frequency is simply the total number of times that a specific word is attested in the corpus. Normalized frequency is a measure of how many times the item appears *for every x tokens* in the corpus. This is usually calculated to be per-million-tokens, though the exact count can vary. Using normalized frequency is more meaningful since it is easier to compare with frequencies found in other corpora.

Frequency forms the core of frequency word lists, and it is also their most simple measure. A word list can be created using frequency alone. However, other measures, such as range, help take into account important factors that frequency ignores.


Gries (2010):
> for example, observed frequencies (or their logs) are good proxies toward the familiarity of words—see Howes and Solomon (1951) for recognition times, Oldfield and Wingfield (1965) as well as Forster and Chambers (1973) for naming times, and Ellis (2002a, b) as well as Jurafsky (2003) and Gilquin and Gries (2009) for overviews.


#### Range

Range is a measure of the number of sub-corpora—or sections of a corpus—in which the word can be found [@FriesEnglishWordLists1960]. Range is also sometimes referred to as *contextual diversity* [@BrysbaertMovingKuceraFrancis2009]. To measure this, a corpus must first be divided into a series of sub-corpora. As of now, there is no real consensus on a specific way to do this, so different word lists may contain very different range measures based on the method chosen by the researcher. Like frequency, range can also be normalized to make the number more meaningful for inter-study comparison.

Nation has gone as far as to suggest that "range figures are more important than frequency figures, because a range figure shows how widely used a word is." [-@NationMakingusingword2016, p. 103] This conclusion is corroborated by studies such as that of Adelman, Brown, and Quesada, which found that range better explained the findings of lexical decision tasks by 1%–3% [@AdelmanContextualdiversitynot2006]. Similar results were found by Ellis, who attributed better predictive power to range than to word frequency [-@EllisFrequencyeffectslanguage2002; -@EllisReflectionsfrequencyeffects2002].

The value of calculating range is that it provides a simple way to evaluate skewed frequency results. For example, a word may be rare overall in a language, but if it happens to be very common in only a few texts, it can still attain an inappropriately high place on the frequency list. This often occurs with specialized words that are only used by a very specific subset of the population but with high frequency. By calculating range, it becomes easy to identify these words.

The question then becomes, what to do once these words are found. How can range and frequency be used in tandem? One possibility, suggested by Nation and used by <!-- examples -->, is to decide on a minimum range, discard any words that fall below this bar, and order only the remaining words by frequency. This approach, however, relies on a subjective decision that becomes diffcult to replicate with other corpora. The fate of words with range close to the cutoff point is to be either completely thrown out or kept in their original position. Shifting the word's position on the list—its rank—is more sensical, but this can quickly become messy and subjective as well. Dispersion tries to solve this problem.


#### Dispersion

In a (simplistic) nutshell, dispersion is a combination of both frequency and range. It serves as a single number—a distributional statistic—that incorporates the benefits of both of these measures, while also allowing a list to be ranked in a methodical, objective manner.

Unfortunately, there is still little agreement on how best to measure dispersion. Many ideas have been proposed, such as Juilland's *D* [@JuillandFrequencydictionaryFrench1970], Carroll's *D~2~* [-@CarrollalternativeJuillandusage1970], Rosengren's *S* [-@Rosengrenquantitativeconceptlanguage1971], Lyne's *D~3~* [-@LyneDispersion1985], and Zhang's *Distributional Consistency* (*DC*) [@ZhangDistributionalconsistencygeneral2004]. A thorough overview of these and other dispersion measures was published by Gries, who then provided his own suggested method, *deviation of proportions*, or *DP* [-@GriesDispersionsadjustedfrequencies2008; -@GriesDispersionsadjustedfrequencies2010; @LijffijtCorrectionStefanTh2012].

Unlike earlier proposals, however, Gries' *DP* stands out as a comparatively simple calculation that takes into account some of the biggest problems he identified in the others. Gries himself lists the benefits of *DP* as: flexibility to use differently sized subcorpora, simplicity, extendability to different scenarios, and appropriate sensitivity.





Brezina and Gablasova (105), p. 8:
> ARF is a measure that takes into account both the absolute frequency of a lexical item and its distribution in the corpus (Savicky´and Hlava´c ˇ ova´2002; Hlava´c ˇ ova´2006). Thus if a word occurs with a relatively high absolute frequency only in a small number of texts, the ARF will be small (cf. Cerma´k and Kr ˇ en 2005; Kilgarriff 2009). All four wordlists were then sorted according to the ARF that ensured that only words that are frequent in a large variety of texts appeared in the top positions in the wordlists.

Sorell (2013), p. 89: Dispersion.



<!--
## Modern non-English word lists


Gardner, D. (2007), p. 242:
> Hazenberg and Hulstijn 1996—Dutch language;

-->
