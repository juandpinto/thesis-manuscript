\newpage

# Background: Review of the literature {#background}

The theoretical foundation of frequency dictionaries—sometimes referred to as frequency lists, word lists, vocabulary lists, and variations thereof—rests on the observation, made popular by the linguist George Kingsley Zipf in the 1930s and 40s, that the first word in any large-enough text occurs roughly twice as often as the second word, three times as often as the third word, and so on [@Zipfpsychobiologylanguage1935; @ZipfHumanbehaviorprinciple1949].

This exponential distribution is significant because it means that a small number of words make up the bulk of a text, whereas the majority of the words occur very few times[@SorellZipflawvocabulary2012]. Paul Nation, one of the most influential scholars in the field of vocabulary acquisition, has pointed out that Zipf’s Law—as it is has come to be known—can serve as motivation to language learners and teachers, since learning the most common vocabulary in a language covers such a large percentage of natural communication [@NationLearningvocabularyanother2013, p. 34].

This observation guides the entire endeavor of frequency dictionary creation and use. Though the *Frequency Dictionary of Spoken Hebrew* is not sorted using raw frequency alone^[The sorting method and key measures used by the FDOSH is explained in detail in the [*objective design*](#objective-design) section of this chapter.], the effect of Zipf's law can be easily seen in the listed frequencies that accompany each item.

Beyond understanding this theoretical basis and its implications, other considerations play an important role in the creation of a frequency dictionary. These include corpus size, corpus text type, whether the list will be for general or specialized use, word family levels, and objective criteria. This literature review will treat each of these themes in turn. Because the most comprehensive studies deal with more than one of these issues, some of them will be brought up at various times to illustrate the point under discussion.


## Corpus design

Before designing a frequency dictionary, a careful plan must be made for the design of the corpus from which the list is extracted. The corpus must be representative of the language context that the dictionary wishes to depict. Of course, capturing that context in its entirety is an impossible feat. For this simple reason, researchers must make do with an approximation of the whole: a bounded corpus of language.

Though the focus of this literature review is the creation of word frequency dictionaries, the truth is that relatively few corpora have been created for this specific purpose. Most corpora have aimed at being general collections that cover the language (usually English) as a whole in an attempt to serve different theoretical and applied uses. Yet despite this broad objective, the creation and use of corpora have historically revolved around two big questions: (1) how large should the corpus be, and (2) what kinds of texts should it include. Both of these issues will be addressed here, with the recurring emphasis being corpus design for frequency list creation.


### Corpus size

Conventional wisdom in corpus creation states that more is better. If a frequency list is to accurately reflect the frequencies of words in the language as a whole, then a corpus must contain enough text to approximate the overall use of discourse. This line of thinking is equivalent to the maxim in quantitative research that a sample should be as representative of the target population as possible. And in order to maximize the statistical probability of this representation, the sample must be of an appropriate size for the study.

True, larger sample sizes often increase this probability, but they also tend to be more resource-intensive for the researcher. The same is true of corpus size. When creating a frequency dictionary, then, what is an "ideal" corpus size?

The first project to create a one-million-token corpus was a joint effort by Henry Kučera and W. Nelson Francis of Brown University to compile a corpus of American English texts printed in 1961 [@KuceraComputationalanalysispresentday1967], known today simply as the *Brown Corpus*. They strived to create a corpus with equal amounts of texts from different sources by randomly selecting 500 passages of 2,000 words each from different published materials found at the Brown University Library and the Providence Athenaeum. This mixed design would be used as a model by many of the corpora created during the next few decades, which began to be compiled at increasingly faster rates.

As an example of how quickly corpora have grown in recent decades, consider the history of COBUILD. What began in 1980 as a collaboration between Collins Publishing and a group of researchers led by John Sinclair—the Collins Birmingham University International Language Database (COBUILD)—led to the creation of the *Collins Corpus* of 7-million-tokens by 1982. It continued expanding until transforming into the *Bank of English* in the 1990s, which reached 320 million words in 1997. In 2005, as part of the Collins World Web, which also comprises French, German, and Spanish corpora, it reached 2.5 billion words [@CollinsCobuildEnglish2005]. The Collins Corpus now contains over 4.5 billion words [@historyCollinsCOBUILD].

Today, with the use of web-crawling applications that scour the internet and collect text at unprecedented speed, the sky's the limit.The *enTenTen12* corpus is composed of 12 billion English tokens, all of which were collected in 12 days [@JakubicekTenTenCorpusFamily2013]! At what point, then is a corpus sufficiently large for frequency-list creation?

Researchers have approached this specific problem by creating multiple frequency lists—from varying sizes of corpora—and then comparing the efficacy of these lists themselves. The way that efficacy is operationalized, however, varies among studies.

Some studies have explored how closely the rankings of items on a word frequency dictionary correlate with reaction times in a lexical decision task—a widely-used procedure in psychological and psycholinguistic research [@OldfieldResponselatenciesnaming1965; @ForsterLexicalaccessnaming1973]. In a lexical decision task, participants are presented with a series of words and non-words, one after the other, and they are asked to judge which is which as quickly as possible. Their reaction times are then analyzed for each word. It is generally agreed that the average time it takes participants to react to a word provides insights into the arrangement of the mental lexicon[@JurafskyProbabilisticmodelingpsycholinguistics2003; @GilquinCorporaexperimentalmethods2009]. For our purposes, multiple studies have found that there exists an inverse correlation between word frequency and reaction time on a lexical decision task [@BalotaAreLexicalDecisions1984; @WhitneyPsychologyLanguage1998]. In other words, more common words are accessed and recognized more quickly than less common words. Therefore, an effective word frequency list should correspond to and reflect this reality.

This was precisely the approach taken by Brysbaert and New [-@BrysbaertMovingKuceraFrancis2009], who compared respond times collected as part of the massive Elexicon Project  [@BalotaEnglishLexiconProject2007] to words on a series of frequency lists made from increasingly larger corpora. The corpora used were all subcorpora extracted from the British National Corpus (BNC). With each subsequent increase in token count, the frequency list correlated more and more closely with the response times from lexical decision tasks. Brysbaert and New hoped to find an “ideal” corpus size, after which the increase in effectiveness would no longer be significant enough to justify the additional cost of resources. After conducting several regression analyses on the two sets of data, they found that the variance in the response times that could be accounted for by corpus size reached a plateau at about 16 million tokens. In other words, for corpora with less than 16 million words, the size of the corpus had a significant effect on the correlation between word frequencies and average response times for those words on lexical decision tasks. For corpora with more than 16 million words, the effect of increasing corpus size became considerably more subtle. In the end, they concluded that in order to construct an effective frequency dictionary for *high-frequency* words, a corpus of about 1–3 million tokens is needed. However, in order to reach the same effectiveness for *low-frequency* words, a corpus size of at least 16 million words is preferable [@BrysbaertMovingKuceraFrancis2009, p. 988].

A different, more straightforward methodology is to directly compare frequency lists made from differently sized corpora. Rather than judging the “effectiveness” of a list, this approach measures similarities shared between different lists. Hypothetically, doing this at increasing corpus sizes should allow one to find a size after which the variance between lists only minimally decreases. As with the previous approach, the goal here is to find a point at which the benefits of increasing size no longer outweigh the needed additional resources.

Essentially, then, all corpora of sufficient size should result in nearly the same frequency dictionary—a theory based on a strict interpretation of Zipf’s law. If the appropriate criteria can be found— Sorell [-@Sorellstudyissuestechniques2013] suggests—then this would, at last, provide a solution to the observation made by Nation [-@NationLearningvocabularyanother2013, p. 24] that, problematically, frequency lists tend to disagree rather drastically on both the words included and their respective ranking.

Inspired by the computational linguistic measure of *rank distance* [@PopescuRankDistanceStylistic2008]—a method for comparing stylistic differences between texts— Sorell [-@Sorellstudyissuestechniques2013] developed a variant of this methodology. First, he used different corpora of the same size to create multiple frequency lists, one for each corpus, ranked entirely by frequency. He then identified the percentage of words that are *not* shared between each set of two lists. Finally, he averaged these percentages to find the level of variability created at that specific corpus size. The levels of variability he found were remarkably close to each other [-@Sorellstudyissuestechniques2013, p. 80]—despite using a wide variety of entirely different corpora (with no overlap on texts within each one). He then increased the size of each corpus and repeated the process.

In order to calculate this level of variability, Sorell used a modified version of a complex formula that he borrowed from the natural sciences, and called his resulting calculation the *Dice distance*. Though this Sørensen–Dice coefficient that he altered is widely used in botany and other fields^[It has even been used in corpus linguistics studies before, primarily as a way to measure collocation [@Rychlylexicographerfriendlyassociationscore2008].] to measure similarity in areas and samples of different sizes [@DiceMeasuresAmountEcologic1945; @Sorensenmethodestablishinggroups1948], the frequency lists measured by Sorell were all purposefully of the same size. What this means is that—apparently without realizing it—his *Dice distance* was ultimately just a simple fraction:

$$\frac{\textbf{number of different words between frequency lists}}{\textbf{total size of frequency list}}$$

In essence, this measure can be accurately described as the average proportion of difference for frequency lists at that particular corpus size.

Sorell found that a stable list (about 2% variation) of the most frequent 1,000 words, or a reasonably stable list (less than 5% variation) of the most frequent 3,000, words can be created using a corpus of 50 million tokens [-@Sorellstudyissuestechniques2013, p. 203]. In other words, 1,000-word frequency lists created from different 50-million-token corpora will likely only differ by 20 words. At the 3,000-word level using the same corpus size, the lists will likely vary by less than 150 words. This is a remarkable level of similarity. Expanding the list to 9,000 words will still only yield about 4–7% variation, or 360–630 words. Even corpora of 20 million tokens can be considered sufficient in many cases, since they will result in 3,000-word frequency lists with roughly 5% variation and 9,000-word frequency lists with less than 10% variation.

Taking a similar comparative approach, Brezina and Gablasova [-@Brezinatherecoregeneral2015] evaluated frequency lists created from four corpora of various sizes: the *Lancaster-Oslo-Bergen Corpus* (LOB), the *BE06 Corpus of British English* (BE06), *The British National Corpus* (BNC), and *EnTenTen16*. These corpora have respective token sizes of 1 million, 1 million, 100 million, and 12 billion. The frequency dictionary created from each corpus was, in this case, ranked by a combination of frequency and dispersion—a measure that will be discussed in more detail in the [*dispersion*](#dispersion) section of this chapter. In addition to finding the percentage of shared items between frequency lists, the researchers calculated the correlations between the rankings for each shared word. Contrary to Sorell [-@Sorellstudyissuestechniques2013], Brezina and Gablasova considered this final comparison an important part of understanding the effect of corpus size.

The aim of this study was not to find a corpus size after which the difference was negligible, but rather to find if there was a significant difference between frequency lists made from corpora of different sizes. The study found a 78%–84% overlap between each of the 3,000–word lists. 71% of the words were shared among all four of the lists. Based on this number, Brezina and Gablasova concluded that regardless of corpus size—at least for anything larger than one million tokens—“similar results” are obtained [-@Brezinatherecoregeneral2015, p. 18].

This conclusion differs significantly from Sorell’s, who concluded that a corpus of at least 20 million tokens (though 50 million is preferable) is needed for a stable frequency list with low variability [@Sorellstudyissuestechniques2013, p. 203]. These disagreements are primarily the result of a difference in what should be considered “stable.” At 71% vocabulary overlap—which is sufficient for Brezina and Gablasova—870 words were only found in one of the four lists. This is drastically higher than Sorell’s threshold, which at the 3,000-word level varies in roughly 150 words. Note that Nation and Kyongho [-@NationWherewouldgeneral1995] found a level of overlap similar to Brezina and Gablasova when comparing the GSL, the LOB, and the Brown corpora—a percentage of overlap that they deemed to be not particularly high. As Nation later put it, “Brezina and Gablasova are a bit too tolerant in accepting that 71% or even 78%-84% overlap is good enough. If roughly one out of every four or five words is different from one list to another, that is a lot of difference” [@NationMakingusingword2016, p. 100].

One issue that has yet to be studied (to my knowledge) is the difference in units of counting between these two studies. Sorell made lists based on *types*, whereas Brezina and Gablasova preferred the use of *lemmas*. The exact difference between these two units is explained later under [*identifying words (word family levels)*](#identifying-words) in this chapter. The effect of these different measures for comparing frequency lists created from differently sized corpora is an area that could benefit from further research.

Regardless of differences between approaches, the studies in this section have demonstrated the importance of having a sufficiently large corpus in order to create a trustworthy frequency dictionary. The next section deals with the second aspect of corpus design: the types of texts that are included.


### Text types

Deciding on the texts that make up a corpus, and their corresponding text types, is a critical aspect of corpus design. Designing a corpus for the goal of creating a frequency dictionary needs to take the dictionary's intended purpose into account. Many corpora take a conglomerate approach, meaning that they simply amass as many texts as possible, regardless of their type. This often results in frequency lists that serve no distinct purpose.

Some published corpora—especially those designed for a specific purpose rather than "core vocabulary" or the language as a whole—do take a more strategic approach. For example, Coxhead’s [-@Coxheadnewacademicword2000] *Academic Word List* was created from a carefully designed corpus that used equally sized subcorpora of texts from different disciplines. This suited the purpose of the frequency list well, since it was intended to serve students from a variety of disciplines.

In order to better understand text types, some studies have sought a taxonomy that would make the selection process more objective. These seek to establish, for example, if there are distinguishable linguistic differences between an informal correspondence and a narrative work of fiction, or between a romance and a fantasy novel.

One influential attempt at this categorization was conducted by Biber [-@BiberVariationspeechwriting1988], who analyzed a variety of texts using large corpora to tag syntactic markers and other linguistic attributes that could potentially be used to define different types of texts. He found a series of five categories (each consisting of two opposite ends of a spectrum) in which texts varied:

1. Involved vs. informational
2. Narrative
3. Situated vs. elaborated
4. Persuasive
5. Abstract

Biber then conducted an in-depth follow-up study that found eight distinct, recurring patterns of different combinations of these categories [-@BiberDimensionsRegisterVariation1995]. These groupings serve as a linguistically-based taxonomy that divides texts along objective lines, rather than subjective, culturally-defined genres.

Similar but independent studies have been conducted for Somali, Korean, Nukulaelae Tuvuluan, Taiwanese, and Spanish [@BiberDimensionsRegisterVariation1995; @JangDimensionsspokenwritten1998]. For each language, a unique set of text types was identified. Significantly, the texts were found to align along similar distinguishing linguistic dimensions as the English texts [@BiberDimensionsRegisterVariation1995, p. 270].

Sorell [-@Sorellstudyissuestechniques2013] sought to simplify Biber’s eight text types into categories suitable for corpus design. He did this by identifying the similar ways that some of the text types lined up along Biber’s five linguistic categories while incorporating some extra-linguistic features, such as shared contexts (e.g. predominantly spoken types). He excluded Biber’s [-@BiberDimensionsRegisterVariation1995] two smallest text types—"situated on-line reportage" and "involved persuasion"—deeming them impractical for corpus study and difficult to isolate [@Sorellstudyissuestechniques2013, p. 68]. In doing this, he came up with four simplified text types:

1. Interactive (conversation)
2. General reported exposition (general writing)
3. Imaginative narrative (narrative writing)
4. Academic

Using his comparison method of Dice distance (described above under [*corpus size*](#corpus-size)), Sorell found each simplified text type to be equidistant from the next in this order: conversation, narrative, general writing, and academic writing [-@Sorellstudyissuestechniques2013, pp. 153–154]. This allowed him to claim that his own study of vocabulary frequency—using simplified text types as a base—has “validated Biber’s studies by adding a vocabulary dimension to the description of each of the key text types” (p. 201).

Similar efforts to simplify Biber's text types have also been carried out in the *Longman Grammar of Spoken and Written English* [@BiberLongmangrammarspoken1999, p. 16] and the *Longman Student Grammar of Spoken and Written English* [@BiberLongmanstudentgrammar2002, p. 23]


#### Conversation text type

Despite the importance of the conversation text type for language learners and linguistic studies, corpora of spoken language remain much smaller than more traditional corpora. This is due to the costs and resources involved with gathering large amounts of spoken data that then need to be transcribed by hand in order to be analyzed. It is true that speech recognition software has come a long way in recent years, but its rate of error remains too high for research purposes. It has been estimated that it takes 40 hours to professionally transcribe one hour of audio recording, making the task too costly [@Newusefilmsubtitles2007, p. 662]. For this reason, some researchers have begun looking at alternative sources of speech corpora, including the internet and movie subtitles [@KilgarriffIntroductionspecialissue2003].

New et al. [-@Newusefilmsubtitles2007] created a 50-million-token corpus of French subtitles. They divided this into four subcorpora, one for each of the type of media from which the subtitles were extracted: French films, English movies, English television series, and non-English-language European films. The reason for using French subtitles from English media is the sheer dominance of English in the film industry. In order to counter-balance the much larger sizes of the two subcorpora extracted from English media, the researchers measured word frequencies for each subcorpora separately, then averaged them to arrive at the final frequency used for their ranked word list.

In order to test the validity of their new approach, New et al. used two different methods. First, they compared their subtitle frequency dictionary with word lists created from more traditional corpora. Second, they used lexical decision times—similar to Brysbaert and New [-@BrysbaertMovingKuceraFrancis2009] above—to test the rankings of words on their list.

The first test found a .73 correlation with another French spoken corpus, the *Corpus de Référence du Français Parlé* (CRFP) [@DelicPresentationCorpusreference2004]. A closer look revealed that the majority of significant differences were caused by the monologue nature of the CRFP. This corpus was created from a large number of interviews (each asking the same questions to the interviewee), whereas movie subtitles tend to be composed primarily of people interacting in conversations. This results in more colloquial expressions having higher frequencies in the subtitle corpus. The nature of movies themselves also played a role, resulting in an overrepresentation of words related to action movies and police matters—words like *tuer* (to kill), *prison* (jail), and *armes* (weapons) [@Newusefilmsubtitles2007, p. 665].

On the second test, New et al. found that their subtitle list’s ability to predict lexical decision times was at least equally as accurate as the CRFP frequencies or those from a traditional corpus of written French [@Newusefilmsubtitles2007, p. 675]. In many cases, it actually fared much better, surprising even the researchers themselves. However, this can only be considered a preliminary finding for two reasons:

1. The sample sizes of the lexical decision task experiments were very small (234 and 240 words).
2. The study's dependence on *translated* subtitles—while understandable given the prevalence of English in the film industry—requires more thorough study before it can be considered a valid alternative. For now, these early findings seem to indicate that it may very well be.^[I deal with a similar limitation of the *Frequency Dictionary of Spoken Hebrew* in the [*methodological challenges*](#methodological-challenges) section of this thesis.]

Picking up on the findings of New et al. [-@Newusefilmsubtitles2007], and expanding the lexical decision task to a much larger sample size, Brysbaert and New [-@BrysbaertMovingKuceraFrancis2009] compiled a corpus of English subtitles (SUBTLEX~US~) and evaluated it as part of their study. This corpus is composed of subtitles from a wide variety of American films since 1900, though a majority are from 1990, as well as a large number of American television series. They found that the subtitle frequencies were especially good at predicting the lexical decision times of short words, often surpassing the accuracy of rankings based on the many written corpora they tested. It had more difficulty explaining the response times of longer words, which are more rarely found in film than in literature. Overall, their own conclusion confirmed that of the New et al. [-@Newusefilmsubtitles2007] study: word frequencies derived from subtitle corpora are as good as—and sometimes better than—those from true speech corpora.

Though both of these studies arrive at the same conclusion regarding the use of subtitles, more research is needed in this area. If, indeed, subtitles can be considered as appropriate sources for corpora of the conversation text type, their availability facilitates the creation of frequency dictionaries of spoken language—something that is otherwise too cost-prohibitive due to the difficulty of the collection medium.

Precisely because of this ease of access, subtitles provide the perfect corpus for a project that seeks to be both representative of *spoken* language and easily reproducible. This is therefore the approach taken in the present thesis to create the *Frequency Dictionary of Spoken Hebrew*. I will give a detailed description of the corpus chosen in the next chapter.


## List design

Perhaps even more complex than appropriately designing the corpus from which to extract word frequencies is designing the frequency dictionary, or list, itself. Many distinct variables are involved in the process. Questions addressed in the literature deal with the difference between a general service list and a specialized list, differences in the way that a “word” is defined and measured, and different ranking criteria used, among other issues.


### General use or specialized use

The majority of frequency dictionaries aim to describe the vocabulary of the language as a whole. They are designed to be all-encompassing so that they can serve any number of uses and scenarios. This broad nature of general-use lists is reflected in the name of the one that has historically been most widely used: West’s *General Service List* (1953). Others include Nation’s *BNC2000* list [-@NationHowlargevocabulary2006], Browne’s *New General Service List* [-@Brownenewgeneralservice2014], Brezina and Gablasova’s *New General Service List* [-@Brezinatherecoregeneral2015], and Dang and Webb’s *Essential Word List* [@NationMakingusingword2016, pp. 153–67].

Another way of understanding general-use lists is that their objective is to find what is often termed the *core* vocabulary. Though not always explicitly stated, the theory behind this approach is that the language contains at its center a self-contained lexicon of essential vocabulary that is fundamental to the entire language. There are layers of frequency and increasing complexity beyond this, with regions of specialized language demarcated for specific purposes such as fields of study or geographically specific dialects. Still, this core vocabulary remains at the center of it all, and the purpose of a frequency dictionary is to identify what words fall within its boundaries. Sorell [-@Sorellstudyissuestechniques2013] has evaluated a number of existing definitions of core vocabulary in the literature.

Fewer researchers have created frequency dictionaries for a more specific purpose or target audience. Specialized-use lists can be designed to only include words that belong to a specific domain, such as a discipline or trade. They can also encompass vocabulary found in a broad range of disciplines, but which are common in a specific context, such as academic texts. In this case, they usually serve as supplements to aid language learners who are already familiar with the "core vocabulary" of the language.

The most well-cited example of a specialized-use list is Coxhead’s *Academic Word List* [-@Coxheadnewacademicword2000], which replaced the *University Word List* [@Xueuniversitywordlist1984] as the go-to vocabulary list for aspiring students intent on attending an English-speaking university or those entering the academic world. This could be considered a *general* academic word list, since it is intended for academic use in general, and not for a specific discipline.

More specialized frequency dictionaries include those designed for business English or medical English courses. This is sometimes designated *technical vocabulary*. Technical vocabulary is most often taught after students have mastered general-use vocabulary, and after they have some familiarity with academic vocabulary [@NationMakingusingword2016]. Chung and Nation [-@ChungTechnicalvocabularyspecialised2003; -@ChungIdentifyingtechnicalvocabulary2004] have analyzed the typical makeup of technical vocabulary. By studying specialized words in the fields of anatomy and applied linguistics, they found that a large number of technical words are also found in the language’s core vocabulary, or have a general academic use as well. However, when used in a technical text, these words often take on a specialized definition that is particular to that domain.

The important takeaway is that the intended purpose of a frequency dictionary needs to be thoroughly considered, since this purpose will affect both the process and outcome of the project. As already mentioned, the *Frequency Dictionary of Spoken Hebrew* is designed for Hebrew learners who wish to focus on *spoken* Hebrew. This narrows the focus from the general core vocabulary of the entire language, but it is not as restricted as a specialized list would be.


### Identifying words (word family levels) {#identifying-words}

Another essential aspect of creating a frequency dictionary is deciding on how to measure a word. Though this may seem like a straightforward task, it requires an understanding of the theory behind the decision. Should *jump* and *jumped* be counted as two different words or just one? What about irregular inflections such as *go* and *went*? In an article aimed at raising awareness of what he calls the “word dilemma,”  Gardner [-@GardnerValidatingconstructword2007] points out that the validity of much vocabulary research hinges “on the various ways that researchers have operationalized the construct of *Word* for counting and analysis purposes” (p. 242).

The literature has generally come to accept some helpful, key terms. Beginning with the most basic measurement and progressing to the most complex, we can choose to count tokens, types, lemmas, or word families.

Consider this example sentence:

> I like small dogs and medium dogs, but even her big dog can be likable.

Measuring *tokens* means simply measuring the total number of words. The example sentence contains fifteen tokens—fifteen words in total. Counting *types* refers to the number of separate and distinct words. That is, *dogs* and *dogs* are the same type, but *dog* is a different type—even a single difference makes them different types. The example sentence is composed of fourteen types. Types are often the simplest measure to use for frequency dictionaries, since they are relatively easy to identify and count.

A *lemma* includes the stem of the word and its inflected forms, but not any derived forms of the word (derived forms are usually considered a different part of speech). So *likes*, *liked*, and *liking* (the verb) are all the same lemma, but *likable* is not. This is because *likable* has the derivational affix *-able*, which turns it into an adjective. Francis et al. define lemma as “a set of lexical forms having the same stem and belonging to the same major word class, differing only in inflection and/or spelling” [@FrancisFrequencyanalysisEnglish1982, p. 1]. The example sentence is made up of thirteen lemmas.

Finally, the term *word family* is used to describe an even more inclusive level than the lemma, though its precise definition has often varied among researchers. Bauer and Nation [-@BauerWordfamilies1993] sought to rectify this problem through an in-depth classification of English affixes. Borrowing from Thorndike’s [-@ThorndiketeachingEnglishsuffixes1941] study of English suffixes, their grouping was based on a series of eight criteria: frequency, productivity, predictability, regularity of the written form of the base, regularity of the spoken form of the base, regularity of the spelling of the affix, regularity of the spoken form of the affix, and regularity of function [@BauerWordfamilies1993, pp. 255–56]. They identified seven “levels” of word families, with each successive one including a larger number of affixes, and therefore a larger number of types per word family. One very useful aspect of their particular system is that it places all the previous levels (type, lemma, etc.) within the same framework. Under their taxonomy, a level 1 word family is the same as a type, a level 2 word family is a lemma (including all regular inflected affixes), and level 7 (the highest level) consists of classical roots and affixes beyond what most speakers any longer consider separate affixes.

Nation himself suggests that for the purpose of language learning, these specific family word levels can be used simply “as a starting point as an initial framework of reference” [@NationMakingusingword2016, p. 36]. That is, they are one interpretation of how to systematically count words for a frequency dictionary. These levels are based on criteria that reflect the needs of language learners, rather than on any psycholinguistic theory of how speakers’ mental lexicon is arranged. Still, the idea of word families aligns closely with theoretical models that dictate morphological decomposition as a constant. These theories propose that words are often deconstructed into independent morphemes in receptive tasks and recognized that way, for example by deconstructing *jumping* into *jump* and *-ing*. At the other end of the spectrum stand theories that would place *jump* and *jumping* as separate lexical entries [@BrysbaertMovingKuceraFrancis2009, pp. 982–83].

Either way, there is strong evidence to suggest that inflected/derived forms and their base forms do affect each other in some way, suggesting that word families are a measure of a real representation in speakers’ mental lexicon. In one such study, Nagy et al. [-@NagyMorphologicalfamiliesinternal1989] explored the effect of both inflectional and derivational family frequency during a lexical decision task. They found that both types of morphological relationships lowered word recognition times, leading to the conclusion that inflections and derivational relationships are both represented in the mental lexicon, either through the grouping of related words under the same entry, or through linked entries. However, all the participants were native English speakers, so to what extent do L2 learners’ lexicons reflect the same level of linking?

More recent studies have found that L2 learners’ morphological knowledge and word-building ability are not nearly as developed. Ward and Chuenjundaeng [-@WardSuffixknowledgeAcquisition2009] conducted a study that tested the receptive ability of Thai engineering and doctoral students learning English. They were tested for their knowledge of a series of base words, together with various derived forms of the same words. They found a surprising lack of familiarity with the derived words, even when participants knew the base forms from which they were derived. Similarly, but from a productive and not receptive standpoint, Schmitt and Zimmerman [-@SchmittDerivativeWordForms2002] found that learners could produce only a limited number of derived forms when presented with a word family headword. These results challenge the common assumption that “once the base word or even a derived word is known, the recognition of other members of the family requires little or no extra effort” [@BauerWordfamilies1993, p. 253].

There is evidence to suggest a positive correlation between vocabulary size and morphological knowledge [@Mochizukiaffixacquisitionorder2000; @SchmittResearchingvocabularyword1997], and between morphological knowledge and reading comprehension [@JeonContributionmorphologicalawareness2011]. If this is the case, then using higher-level word families to create frequency dictionaries, such as Nation’s *BNC2000* list [-@NationHowlargevocabulary2006] or Coxhead’s *Academic Word List* [-@Coxheadnewacademicword2000], may not be appropriate for learners with limited knowledge of vocabulary—the very learners that many of these lists target.

When creating a frequency dictionary, then, the unit of word counting needs to suit the list's purpose and target audience. Brezina and Gablasova [-@Brezinatherecoregeneral2015] contend that Bauer and Nation’s [-@BauerWordfamilies1993] higher word family levels ignore the lack of transparency that exists between many of the entries that would be placed under the same word family. This is especially troublesome when used in frequency lists for language learners, whose morphological knowledge is often not well developed. Because their *New General Service List* was created for beginners, and since it is intended to aid vocabulary acquisition for both receptive and productive purposes, Brezina and Gablasova chose the lemma as their unit of measure.

Given the similar target audience, and using the same reasoning as Brezina and Gablasova [-@Brezinatherecoregeneral2015], I have chosen to use lemmas as the word unit to measure in creating the *Frequency Dictionary of Spoken Hebrew*.

One last note regarding Bauer and Nation's [-@BauerWordfamilies1993] word family levels: they are specific to English. Because they are based entirely on affixation of morphemes, they cannot be readily applied to other languages. Whereas types and lemmas can be more easily understood to be equivalent across languages (with some deviation for highly agglutinative or synthetic languages), extending the concept of word family levels beyond English requires creating a similar taxonomy for each specific language. This is an area of study that has yet to receive more attention.


### Objective design

Many frequency dictionaries—including some of the most widely-known ones—take what could be called a semi-objective approach. They begin by creating a list that bases word rankings on statistical measures such as frequency, range, and dispersion. Then, because certain words don't fit the researcher's intuitions, or because some rankings simply seem out of order, the list is tweaked here and there [@NationMakingusingword2016, p. 133].

For example, one common tweak is to group lexical sets together on a list, such as days of the week or numbers [@NationMakingusingword2016, pp. 118–119]. This is true of West's GSL, resulting in a list that "brought a large element of subjectivity into the final product" [@Brezinatherecoregeneral2015, p. 3]. West himself laid out his argument as to why he chose to use such an approach [@Westgeneralservicelist1953, pp. ix–x]. Nation has also defended the use of subjective criteria [-@NationMakingusingword2016, pp. 119–120].

Despite a few pedagogical advantages, however, a semi-objective approach (which is therefore also a semi-subjective approach) has important implications for reproducibility. This alone makes it unfit for the present project, since one of the primary goals of this thesis is to present an easily reproducible process that can be used to create frequency dictionaries in many different languages. Additionally, the simple fact is that by inserting subjective criteria into the list-creation process, it ceases to be based on the data directly. Rather than letting a particular corpus speak for itself, the whims and opinions of the researcher come into play. This can affect secondary tests that may be performed using the list, such as a lexical decision test.

Some frequency dictionaries that use strictly objective criteria include *Word Frequencies in Written and Spoken English* [@LeechWordFrequenciesWritten2001], Brezina and Gablasova's *New General Service List* [-@Brezinatherecoregeneral2015], and Dang and Webb's *Essential Word List* [@NationMakingusingword2016, pp. 153–67]. This thesis also uses exclusively objective criteria to create the *Frequency Dictionary of Spoken Hebrew*: frequency, range, and dispersion. I will now discuss each of these in turn.


#### Frequency

Frequency can refer to either raw frequency (sometimes called absolute frequency) or normalized frequency. Raw frequency is simply the total number of times that a specific word is attested in the corpus. Normalized frequency is a measure of how many times the item appears *for every x tokens* in the corpus. This is usually calculated to be per-million-tokens, though the exact count can vary. Using normalized frequency is more meaningful since it is easier to compare with frequencies found in other corpora.

Frequency forms the core of frequency dictionaries, and it is also their most simple measure. A word list can be created using frequency alone. However, other measures, such as range, help take into account important factors that frequency ignores.


#### Range

Range is a measure of the number of sub-corpora—or sections of a corpus—in which the word can be found [@FriesEnglishWordLists1960]. Range is also sometimes referred to as *contextual diversity* [@BrysbaertMovingKuceraFrancis2009]. To measure this, a corpus must first be divided into a series of sub-corpora. As of now, there is no real consensus on a specific way to do this, so different frequency dictionaries may contain very different range measures based on the method chosen by the researcher. Like frequency, range can also be normalized to make the number more meaningful for inter-study comparison.

Nation has gone as far as to suggest that "range figures are more important than frequency figures, because a range figure shows how widely used a word is" [@NationMakingusingword2016, p. 103]. This conclusion is corroborated by studies such as that of Adelman et al. [-@AdelmanContextualdiversitynot2006], which found that range better explained the findings of lexical decision tasks by 1%–3%. Similar results were found by Ellis, who attributed better predictive power to range than to word frequency [@EllisFrequencyeffectslanguage2002; @EllisReflectionsfrequencyeffects2002].

The value of calculating range is that it provides a simple way to evaluate skewed frequency results. For example, a word may be rare overall in a language, but if it happens to be very common in only a few texts, it can still attain an inappropriately high place on the frequency list. This often occurs with specialized words that are only used by a very specific subset of the population but with high frequency. By calculating range, it becomes easy to identify these words.

The question then becomes, what to do once these words are found. How can range and frequency be used in tandem? One possibility, suggested by Nation [-@NationMakingusingword2016, pp. 121–122] and used by Coxhead [-@Coxheadnewacademicword2000], is to decide on a minimum range, discard any words that fall below this threshold, and rank only the remaining words by frequency. This approach, however, relies on a subjective decision that becomes difficult to replicate with other corpora. The fate of words with range measures close to the cutoff point is to be either completely thrown out or kept in their original position. Shifting the word's position on the list—its rank—is more sensical, but this can quickly become messy and subjective as well. Dispersion tries to solve this problem.


#### Dispersion

In a (simplistic) nutshell, dispersion is a combination of both frequency and range. It serves as a single number—a distributional statistic—that incorporates the benefits of both of these measures, while also allowing a list to be ranked in a methodical, objective manner.

Whereas frequency and range are found simply by counting, dispersion requires a calculation that incorporates multiple variables. Unfortunately, there is still little agreement on how best to measure dispersion. Many ideas have been proposed, such as Juilland's *D* [@JuillandFrequencydictionaryFrench1970], Carroll's *D~2~* [-@CarrollalternativeJuillandusage1970], Rosengren's *S* [-@Rosengrenquantitativeconceptlanguage1971], Lyne's *D~3~* [-@LyneDispersion1985], and Zhang's *Distributional Consistency* (*DC*) [@ZhangDistributionalconsistencygeneral2004]. One additional measure, *Average Reduced Frequency* or *ARF* [@SavickyMeasureswordcommonness2002; @HlavacovaNewapproachfrequency2006] was used by Brezina and Gablasova to create the *New General Service List* [-@Brezinatherecoregeneral2015, p. 8] mentioned above. *ARF* takes a different approach, in that it sees the entire corpus as one long string of text rather than a series of subcorpora.

A thorough overview of all these and more dispersion measures was published by Gries, who then provided his own suggested method: *deviation of proportions*, or *DP* [@GriesDispersionsadjustedfrequencies2008; @GriesDispersionsadjustedfrequencies2010]. Unlike earlier proposals, however, Gries's *DP* stands out as a comparatively simple calculation that takes into account some of the biggest shortcomings he identified in the others. Gries himself lists the advantages of *DP* as: flexibility to use differently sized subcorpora, simplicity, extendability to different scenarios, and appropriate sensitivity.

The idea behind *DP* is simple. For each word, it aims to find the difference between the frequency one would expect to find in each subcorpus (if the word was perfectly evenly distributed) and the word's actual frequency. Finding the sum of the absolute values of all these distances from perfect dispersion, and then dividing the result in half (since the differences are found in both directions—higher and lower frequencies than expected), one is left with a value between 0 and 1. A *DP* of 0 represents a perfectly even dispersion, and a *DP* close to 1 means a more uneven distribution, where fewer subcorpora contain a larger load of the word's overall frequency. A *DP* of 1 is not actually possible, though Gries explains how to use a normalized value, *DP~norm~*, for those who prefer a true 0–1 range [@GriesDispersionsadjustedfrequencies2008, p. 419; @LijffijtCorrectionStefanTh2012]. The entire equation looks like this:

$$DP\ =\ 0.5 \sum_{i=1}^{n} \left|\ \frac{tokens\ in\ subcorpus_i}{tokens\ in\ corpus}\ -\ \frac{frequency\ of\ lemma_x\ in\ subcorpus_i}{frequency\ of\ lemma_x\ in\ corpus}\ \right|$$

Because frequency does not play a direct role in calculating *DP*, Gries suggests—as a quick fix—using the product of *DP* and frequency [@GriesDispersionsadjustedfrequencies2008, p. 426]. This is similar to previous adjusted frequency measures such as Juilland's [-@JuillandFrequencydictionaryFrench1970] usage coefficient *U*. Gries goes on to explain how his proposed *U~DP~* may obscure what is actually being measured. However, he does not elaborate on a better measure that could be used to rank items on a frequency dictionary. *U~DP~*, therefore, continues to be used by for this purpose [@Sorellstudyissuestechniques2013, p. 89; @MatsushitaWhatOrderShould2012, p. 99]. It is also the ranking measure used to create the *Frequency Dictionary of Spoken Hebrew*.


## Summary and applications

This literature review has outlined some of the most pressing issues that must be considered when creating a word frequency dictionary. As we have seen, research into some of these questions has led to general agreement, in other areas the research is only beginning, and a few issues have generated much discussion but still no true consensus. This overview has laid the groundwork for the decisions that underlie the methods used to create the *Frequency Dictionary of Spoken Hebrew*.

On the matter of corpus design, I have chosen to work with a corpus of *at least* 20 million tokens, and preferably 50 million, in accordance with Sorell's [-@Sorellstudyissuestechniques2013] findings. As for the corpus's text type, because the FDOSH aims to be a list based on interpersonal interactions, it is created entirely from a *conversation* corpus.

Though not a true core vocabulary list, the FDOSH has been created to serve as a foundation for learners of Hebrew, with the goal of reaching conversational proficiency in a wide range of areas, rather than in a specific discipline or setting. Due to the lack of large, high-quality corpora of spoken Hebrew, the FDOSH is based on a corpus of film subtitles. This approach is justified by the findings of studies that compare subtitle corpora to traditional corpora of spoken language, though this area of research is admittedly in need of further study [@BrysbaertMovingKuceraFrancis2009; @Newusefilmsubtitles2007]. The specific details of the corpus used for the FDOSH will be addressed more in depth in the following chapter.

Because the FDOSH is designed primarily for language learners, Bauer and Nation's [-@BauerWordfamilies1993] higher word family levels were deemed inappropriate, based on evidence of learners' weak morphological knowledge and word-building ability [@WardSuffixknowledgeAcquisition2009; @Brezinatherecoregeneral2015; @Mochizukiaffixacquisitionorder2000; @SchmittResearchingvocabularyword1997]. Instead, it uses the lemma—or level 2 in Bauer and Nation's taxonomy—in its counting and arrangement.

Finally, the FDOSH seeks to establish an entirely objective approach to frequency dictionary creation. It does this by ranking words based on a usage coefficient of Gries's deviation of proportions, or *U~DP~* [@GriesDispersionsadjustedfrequencies2008; @GriesDispersionsadjustedfrequencies2010]. This allows for all three key factors of frequency, range, and dispersion to play a role in deciding the order of the words. The FDOSH also includes normalized frequency and range for each item.
